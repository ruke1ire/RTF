# @package _global_
defaults:
  - /experiment/wt103/base.yaml
  - /model/layer: h3

model:
  _name_: lm
  d_model: 768
  n_layer: 12
  d_inner: ${eval:4 * ${.d_model}}
  vocab_size: 50257
  resid_dropout: 0.0
  embed_dropout: 0.1
  layer:
    use_fast_fftconv: True
  attn_layer_idx: [1, 8]
  attn_cfg:
    num_heads: 12
    use_flash_attn: True
    fused_bias_fc: True
    dropout: 0.1
  fused_mlp: True
  fused_dropout_add_ln: True
  residual_in_fp32: True
  pad_vocab_size_multiple: 8
